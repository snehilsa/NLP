{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Full+data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra=pd.read_csv('extra_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes=pd.read_excel('Womens+Attributes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_product_attributes=pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes=attributes[['Style',\n",
    "       '(Reference Style Lookbook, choose all that apply)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DVBTBPHR8WJTCVEN5AJRHF47</td>\n",
       "      <td>01DVBTBPJ41VVT00JJCG8TTZ2W</td>\n",
       "      <td>gender</td>\n",
       "      <td>Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DVA7QRXM928ZM0WWR7HFNTC1</td>\n",
       "      <td>01DVA7QRXXR9F0TWVE1HMC5ZQ3</td>\n",
       "      <td>Primary Color</td>\n",
       "      <td>Blacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DPGV4YRP3Z8J85DASGZ1Y99W</td>\n",
       "      <td>01DPGVGBK6YGNYGNF2S6FSH02T</td>\n",
       "      <td>style</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01E1JM43NQ3H17PB22EV3074NX</td>\n",
       "      <td>01E1JM5WFWWCCCH3JTTTCYQCEQ</td>\n",
       "      <td>style</td>\n",
       "      <td>Modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DSE8Z2ZDAZKZ2SKCS1E3B3HK</td>\n",
       "      <td>01DSE8ZG8Y3FR8KWE2TY1QDWBF</td>\n",
       "      <td>shoe_width</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id            product_color_id attribute_name  \\\n",
       "0  01DVBTBPHR8WJTCVEN5AJRHF47  01DVBTBPJ41VVT00JJCG8TTZ2W         gender   \n",
       "1  01DVA7QRXM928ZM0WWR7HFNTC1  01DVA7QRXXR9F0TWVE1HMC5ZQ3  Primary Color   \n",
       "2  01DPGV4YRP3Z8J85DASGZ1Y99W  01DPGVGBK6YGNYGNF2S6FSH02T          style   \n",
       "3  01E1JM43NQ3H17PB22EV3074NX  01E1JM5WFWWCCCH3JTTTCYQCEQ          style   \n",
       "4  01DSE8Z2ZDAZKZ2SKCS1E3B3HK  01DSE8ZG8Y3FR8KWE2TY1QDWBF     shoe_width   \n",
       "\n",
       "  attribute_value  \n",
       "0           Women  \n",
       "1          Blacks  \n",
       "2          Casual  \n",
       "3          Modern  \n",
       "4          Medium  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USC_product_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped all columns with ALL null values\n",
    "data_dropped=data.dropna(axis=1,how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach additional data to full data\n",
    "full_data=pd.concat([data_dropped,extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=pd.merge(full_data,USC_product_attributes,on='product_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87229"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87229"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No duplicate rows\n",
    "len(final_data.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5655 rows have attribute occasion\n",
    "#final_data[final_data['attribute_name']=='occasion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Weekend', 'Work', 'Day to Night', 'Night Out', 'Vacation',\n",
       "       'Workout'], dtype=object)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See how many occasion exists, These are the occasions we need to look for and find rules for\n",
    "final_data[final_data['attribute_name']=='occasion']['attribute_value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start with Night Out - What clothing items are tagged as night\n",
    "night_out=final_data[final_data['attribute_value']=='Night Out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "night_out_subset=night_out[['brand','brand_category','description','details','product_id','name','tsv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # % each word comes\n",
    "# night_out_subset['brand'].value_counts(1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Night_out_target']=np.where(final_data['attribute_value']=='Night Out',1,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model=final_data[['brand_category','description','details','name','product_id','tsv','Night_out_target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove / from brand_category by space\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace(',',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('\\n',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model.head()\n",
    "subset_for_model.fillna('Empty',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehilsaraswat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "columns=['details','description','name','brand_category']\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0, 10):\n",
    "        new=re.sub('[^a-zA-Z]', ' ',str(subset_for_model.loc[i,j]))\n",
    "        new = new.lower()\n",
    "        new = new.split()\n",
    "        ps = PorterStemmer()\n",
    "        new = [ps.stem(word) for word in new if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        subset_for_model.loc[i,j]=new\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a bag of words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_category</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>name</th>\n",
       "      <th>product_id</th>\n",
       "      <th>tsv</th>\n",
       "      <th>Night_out_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>themensstor shoe sneaker lowtop</td>\n",
       "      <td>vintag fit leather sneaker logo print design s...</td>\n",
       "      <td>themensstor shoe sneaker lowtop</td>\n",
       "      <td>origin fit sneaker</td>\n",
       "      <td>01DSRPSZTDW2PGK1YWYXJGKZZ0</td>\n",
       "      <td>'design':12 'fila':1A 'fit':3A,6 'leather':7 '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unknown</td>\n",
       "      <td>empti</td>\n",
       "      <td>unknown</td>\n",
       "      <td>hat</td>\n",
       "      <td>01DSQXJBX0R7DCW7KTAC1SW547</td>\n",
       "      <td>'chanel':1A 'hat':2A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accessori</td>\n",
       "      <td>timeless leather belt craft smooth cowhid mini...</td>\n",
       "      <td>accessori</td>\n",
       "      <td>petit oval buckl belt</td>\n",
       "      <td>01DPGV8TGRAB993PF7Z3YWG2VR</td>\n",
       "      <td>'belt':5A,9 'buckl':4A,21 'cowhid':13 'craft':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>justkid girl girl swimwearcoverup justkid girl...</td>\n",
       "      <td>pretti ruffl sleev trim elev essenti tank swim...</td>\n",
       "      <td>justkid girl girl swimwearcoverup justkid girl...</td>\n",
       "      <td>littl gir girl ariana one piec upf swimsuit</td>\n",
       "      <td>01DSR8G3F7DBRTMP8THF97XSQ2</td>\n",
       "      <td>'50':14A 'allov':28 'ariana':9A 'color':27 'el...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>justkid babi month infantgirl footiesromp</td>\n",
       "      <td>versatil convert gown eleph appliqu</td>\n",
       "      <td>justkid babi month infantgirl footiesromp</td>\n",
       "      <td>babi girl endear eleph pima cotton convert gown</td>\n",
       "      <td>01DSR8G5GP519DEDCSKBMWQVK5</td>\n",
       "      <td>'appliqu':17 'babi':3A 'convert':10A,13 'cotto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      brand_category  \\\n",
       "0                    themensstor shoe sneaker lowtop   \n",
       "1                                            unknown   \n",
       "2                                          accessori   \n",
       "3  justkid girl girl swimwearcoverup justkid girl...   \n",
       "4          justkid babi month infantgirl footiesromp   \n",
       "\n",
       "                                         description  \\\n",
       "0  vintag fit leather sneaker logo print design s...   \n",
       "1                                              empti   \n",
       "2  timeless leather belt craft smooth cowhid mini...   \n",
       "3  pretti ruffl sleev trim elev essenti tank swim...   \n",
       "4                versatil convert gown eleph appliqu   \n",
       "\n",
       "                                             details  \\\n",
       "0                    themensstor shoe sneaker lowtop   \n",
       "1                                            unknown   \n",
       "2                                          accessori   \n",
       "3  justkid girl girl swimwearcoverup justkid girl...   \n",
       "4          justkid babi month infantgirl footiesromp   \n",
       "\n",
       "                                              name  \\\n",
       "0                               origin fit sneaker   \n",
       "1                                              hat   \n",
       "2                            petit oval buckl belt   \n",
       "3      littl gir girl ariana one piec upf swimsuit   \n",
       "4  babi girl endear eleph pima cotton convert gown   \n",
       "\n",
       "                   product_id  \\\n",
       "0  01DSRPSZTDW2PGK1YWYXJGKZZ0   \n",
       "1  01DSQXJBX0R7DCW7KTAC1SW547   \n",
       "2  01DPGV8TGRAB993PF7Z3YWG2VR   \n",
       "3  01DSR8G3F7DBRTMP8THF97XSQ2   \n",
       "4  01DSR8G5GP519DEDCSKBMWQVK5   \n",
       "\n",
       "                                                 tsv  Night_out_target  \n",
       "0  'design':12 'fila':1A 'fit':3A,6 'leather':7 '...                 0  \n",
       "1                               'chanel':1A 'hat':2A                 0  \n",
       "2  'belt':5A,9 'buckl':4A,21 'cowhid':13 'craft':...                 0  \n",
       "3  '50':14A 'allov':28 'ariana':9A 'color':27 'el...                 0  \n",
       "4  'appliqu':17 'babi':3A 'convert':10A,13 'cotto...                 0  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_for_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model['Mix_of_items']=subset_for_model['brand_category']+subset_for_model['description']+subset_for_model['name']+subset_for_model['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in columns:\n",
    "#     cv=CountVectorizer(max_features = 1500)\n",
    "#     X=cv.fit_transform(subset_for_model).toarray()\n",
    "#     y = subset_for_model.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=subset_for_model['Mix_of_items']\n",
    "y = subset_for_model.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "vect=CountVectorizer(max_features=1500)\n",
    "X=vect.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-492-3a848b4cf63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_NB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred_NB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0;32m--> 452\u001b[0;31m                                  (self.sigma_[i, :]), 1)\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred_NB = classifier.predict(X_test.toarray())\n",
    "y_pred_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29495,  5139],\n",
       "       [   48,   210]])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_NB = confusion_matrix(y_test, y_pred_NB) \n",
    "cm_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.13412816691506"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "(29495+210)/34892*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813953488372093"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210/(210+48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on Full Data for occasions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion=final_data[final_data['attribute_name']=='occasion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion['brand_category']=occasion['brand_category'].str.replace('/',' ')\n",
    "occasion['brand_category']=occasion['brand_category'].str.replace(',',' ')\n",
    "occasion['details']=occasion['brand_category'].str.replace('\\n',' ')\n",
    "occasion['details']=occasion['brand_category'].str.replace('/',' ')\n",
    "occasion['details']=occasion['brand_category'].str.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion.head()\n",
    "occasion.fillna('Empty',inplace=True)\n",
    "occasion=occasion.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehilsaraswat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "columns=['details','description','name','brand_category']\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0, len(occasion)):\n",
    "        new=re.sub('[^a-zA-Z]', ' ',str(occasion.loc[i,j]))\n",
    "        new = new.lower()\n",
    "        new = new.split()\n",
    "        ps = PorterStemmer()\n",
    "        new = [ps.stem(word) for word in new if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        occasion.loc[i,j]=new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata=occasion[['brand_category','description','details','name']]\n",
    "y = occasion['attribute_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "vect=CountVectorizer(max_features=1500)\n",
    "X=sp.hstack(fdata.apply(lambda col: vect.fit_transform(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Day to Night', 'Workout', 'Day to Night', ..., 'Weekend',\n",
       "       'Day to Night', 'Vacation'], dtype='<U12')"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_NB = classifier.predict(X_test.toarray())\n",
    "y_pred_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3169761273209549"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred_NB,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Predicting with this shitty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bc_product_id                                                        NaN\n",
       "brand                                                   Emilia Wickstead\n",
       "brand_canonical_url    https://www.modaoperandi.com/emilia-wickstead-...\n",
       "brand_category                                      women:CLOTHING:PANTS\n",
       "created_at                                 2019-11-07 19:40:24.663272+00\n",
       "description            Emilia Wickstead' proves that her tailoring is...\n",
       "details                Concealed zip fastening along back\\nCompositio...\n",
       "labels                                                                {}\n",
       "mpn                                                               347393\n",
       "name                          Francis Belted Checked Wool Wide-Leg Pants\n",
       "notes                                                                NaN\n",
       "product_id                                    01DS3P3NPXE2R3SFYNJNMCMSGZ\n",
       "saleable                                                             NaN\n",
       "tsv                    'belt':4A,49 'check':5A,34,36 'covet':20 'crea...\n",
       "updated_at                                 2019-12-19 20:40:30.786144+00\n",
       "product_color_id                              01DS3P3NQ21PTRVZXF5ZJ6N0CG\n",
       "attribute_name                                                  occasion\n",
       "attribute_value                                             Day to Night\n",
       "Night_out_target                                                       0\n",
       "Name: 153, dtype: object"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[final_data['attribute_name']=='occasion'].iloc[18,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter descriptionEmilia Wickstead' proves that her tailoring is just as covetable as the label's glamorous evening dresses. The label's 'Francis' pants are checked from checked virgin wool that's outlined in smart creases. Use the wide waist belt to temper the cuffed wide-leg hem\n",
      "Please Enter Brandemi\n",
      "Enter categorywomen:CLOTHING:PANTS\n"
     ]
    }
   ],
   "source": [
    "#Take user input\n",
    "description=input(\"Please Enter description\")\n",
    "Brand=input(\"Please Enter Brand\")\n",
    "category=input(\"Enter category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "description=description.replace('\\n',' ')\n",
    "description=description.replace('/',' ')\n",
    "description=description.replace(',',' ')\n",
    "category=category.replace('\\n',' ')\n",
    "category=category.replace('/',' ')\n",
    "category=category.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehilsaraswat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "columns=[description,category]\n",
    "for j in range(0,len(columns)):\n",
    "    corpus = []\n",
    "    for i in range(0,1):\n",
    "        new=re.sub('[^a-zA-Z]', ' ',columns[j])\n",
    "        new = new.lower()\n",
    "        new = new.split()\n",
    "        ps = PorterStemmer()\n",
    "        new = [ps.stem(word) for word in new if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        columns[j]=new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emilia wickstead prove tailor covet label glamor even dress label franci pant check check virgin wool outlin smart creas use wide waist belt temper cuf wide leg hem',\n",
       " 'women cloth pant']"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "X=vect.transform(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x987 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ace',\n",
       " 'acid',\n",
       " 'acryl',\n",
       " 'adler',\n",
       " 'agn',\n",
       " 'air',\n",
       " 'airbrush',\n",
       " 'airi',\n",
       " 'airlift',\n",
       " 'al',\n",
       " 'alameda',\n",
       " 'alex',\n",
       " 'alexia',\n",
       " 'alger',\n",
       " 'alli',\n",
       " 'alloisa',\n",
       " 'alosoft',\n",
       " 'alpaca',\n",
       " 'ama',\n",
       " 'amari',\n",
       " 'amor',\n",
       " 'andi',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'angelica',\n",
       " 'ankl',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annabel',\n",
       " 'anorak',\n",
       " 'ansel',\n",
       " 'anson',\n",
       " 'appliqu',\n",
       " 'arabel',\n",
       " 'arizona',\n",
       " 'arliss',\n",
       " 'arlo',\n",
       " 'around',\n",
       " 'ashlynn',\n",
       " 'aslen',\n",
       " 'asymmetr',\n",
       " 'athena',\n",
       " 'atla',\n",
       " 'aubre',\n",
       " 'aura',\n",
       " 'aurora',\n",
       " 'avelin',\n",
       " 'avenu',\n",
       " 'babi',\n",
       " 'back',\n",
       " 'backpack',\n",
       " 'bag',\n",
       " 'bailey',\n",
       " 'balloon',\n",
       " 'bambi',\n",
       " 'banner',\n",
       " 'barbara',\n",
       " 'bardot',\n",
       " 'bare',\n",
       " 'bastian',\n",
       " 'bb',\n",
       " 'bea',\n",
       " 'beachcomb',\n",
       " 'beachwear',\n",
       " 'bead',\n",
       " 'bedford',\n",
       " 'bell',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'benelli',\n",
       " 'bent',\n",
       " 'beverli',\n",
       " 'beya',\n",
       " 'bi',\n",
       " 'bike',\n",
       " 'biker',\n",
       " 'bindl',\n",
       " 'birkenstock',\n",
       " 'birkin',\n",
       " 'bissett',\n",
       " 'black',\n",
       " 'blackcomb',\n",
       " 'blake',\n",
       " 'blanc',\n",
       " 'blazer',\n",
       " 'blend',\n",
       " 'block',\n",
       " 'blous',\n",
       " 'blue',\n",
       " 'boardwalk',\n",
       " 'bodysuit',\n",
       " 'boilersuit',\n",
       " 'bomber',\n",
       " 'bon',\n",
       " 'bond',\n",
       " 'boot',\n",
       " 'booti',\n",
       " 'border',\n",
       " 'bound',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boxi',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'bra',\n",
       " 'bradner',\n",
       " 'breast',\n",
       " 'brenda',\n",
       " 'brent',\n",
       " 'briana',\n",
       " 'briefcas',\n",
       " 'brixton',\n",
       " 'brocad',\n",
       " 'brush',\n",
       " 'brushstrok',\n",
       " 'bucket',\n",
       " 'buckl',\n",
       " 'burgo',\n",
       " 'bust',\n",
       " 'button',\n",
       " 'cabin',\n",
       " 'cabinet',\n",
       " 'cabl',\n",
       " 'cabria',\n",
       " 'cadel',\n",
       " 'caden',\n",
       " 'cadi',\n",
       " 'calf',\n",
       " 'cali',\n",
       " 'calixt',\n",
       " 'camera',\n",
       " 'cameron',\n",
       " 'cami',\n",
       " 'camisol',\n",
       " 'camo',\n",
       " 'camouflag',\n",
       " 'cancun',\n",
       " 'canva',\n",
       " 'cap',\n",
       " 'cape',\n",
       " 'capri',\n",
       " 'cardigan',\n",
       " 'cargo',\n",
       " 'carin',\n",
       " 'carmen',\n",
       " 'carpenteria',\n",
       " 'carr',\n",
       " 'cashmer',\n",
       " 'celest',\n",
       " 'celin',\n",
       " 'chain',\n",
       " 'chambray',\n",
       " 'charley',\n",
       " 'charli',\n",
       " 'charlott',\n",
       " 'charmeus',\n",
       " 'chase',\n",
       " 'check',\n",
       " 'chenil',\n",
       " 'cheyenn',\n",
       " 'chiara',\n",
       " 'chiffon',\n",
       " 'chine',\n",
       " 'chinook',\n",
       " 'chloe',\n",
       " 'christabella',\n",
       " 'christoph',\n",
       " 'chuck',\n",
       " 'chunki',\n",
       " 'cigarett',\n",
       " 'cinch',\n",
       " 'citi',\n",
       " 'clair',\n",
       " 'clara',\n",
       " 'clarita',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clima',\n",
       " 'climb',\n",
       " 'cloqu',\n",
       " 'clutch',\n",
       " 'coat',\n",
       " 'cobra',\n",
       " 'collar',\n",
       " 'collarless',\n",
       " 'collect',\n",
       " 'collin',\n",
       " 'color',\n",
       " 'colorblock',\n",
       " 'colroy',\n",
       " 'combat',\n",
       " 'combin',\n",
       " 'constanc',\n",
       " 'contrast',\n",
       " 'convert',\n",
       " 'copain',\n",
       " 'corben',\n",
       " 'corduroy',\n",
       " 'corner',\n",
       " 'corsa',\n",
       " 'cortessa',\n",
       " 'cortez',\n",
       " 'cosima',\n",
       " 'cotton',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'cowl',\n",
       " 'crackl',\n",
       " 'crepe',\n",
       " 'crew',\n",
       " 'crewneck',\n",
       " 'crinkl',\n",
       " 'croc',\n",
       " 'crochet',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'cruz',\n",
       " 'crystal',\n",
       " 'curb',\n",
       " 'curvi',\n",
       " 'custella',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cutout',\n",
       " 'cyrena',\n",
       " 'dahlia',\n",
       " 'danni',\n",
       " 'dark',\n",
       " 'dawson',\n",
       " 'day',\n",
       " 'dazzler',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deana',\n",
       " 'debbi',\n",
       " 'dede',\n",
       " 'deep',\n",
       " 'delilah',\n",
       " 'delray',\n",
       " 'demi',\n",
       " 'demna',\n",
       " 'demyle',\n",
       " 'deneuv',\n",
       " 'denim',\n",
       " 'dessi',\n",
       " 'destin',\n",
       " 'detail',\n",
       " 'devon',\n",
       " 'diamond',\n",
       " 'diana',\n",
       " 'dickey',\n",
       " 'dillon',\n",
       " 'dimens',\n",
       " 'dip',\n",
       " 'dirusa',\n",
       " 'distress',\n",
       " 'district',\n",
       " 'doey',\n",
       " 'dojo',\n",
       " 'doll',\n",
       " 'dome',\n",
       " 'dominica',\n",
       " 'donita',\n",
       " 'dot',\n",
       " 'doubl',\n",
       " 'drama',\n",
       " 'drape',\n",
       " 'drapey',\n",
       " 'drawstr',\n",
       " 'dreami',\n",
       " 'dress',\n",
       " 'dri',\n",
       " 'drop',\n",
       " 'dryden',\n",
       " 'dua',\n",
       " 'dualiti',\n",
       " 'duchess',\n",
       " 'duffel',\n",
       " 'dy',\n",
       " 'dye',\n",
       " 'easi',\n",
       " 'eddi',\n",
       " 'edg',\n",
       " 'edit',\n",
       " 'edna',\n",
       " 'effect',\n",
       " 'eight',\n",
       " 'elev',\n",
       " 'elisa',\n",
       " 'ella',\n",
       " 'elyanna',\n",
       " 'embellish',\n",
       " 'emboss',\n",
       " 'embrac',\n",
       " 'embroid',\n",
       " 'emilia',\n",
       " 'envelop',\n",
       " 'eo',\n",
       " 'erin',\n",
       " 'espadril',\n",
       " 'essenti',\n",
       " 'est',\n",
       " 'eudora',\n",
       " 'eva',\n",
       " 'everli',\n",
       " 'everyday',\n",
       " 'ex',\n",
       " 'exclus',\n",
       " 'eyelet',\n",
       " 'fabot',\n",
       " 'face',\n",
       " 'fagan',\n",
       " 'faill',\n",
       " 'fall',\n",
       " 'farrel',\n",
       " 'faux',\n",
       " 'favourit',\n",
       " 'fawn',\n",
       " 'fei',\n",
       " 'femm',\n",
       " 'ferlyn',\n",
       " 'fever',\n",
       " 'figu',\n",
       " 'figura',\n",
       " 'fila',\n",
       " 'film',\n",
       " 'fiona',\n",
       " 'fishtail',\n",
       " 'fit',\n",
       " 'flannel',\n",
       " 'flare',\n",
       " 'flat',\n",
       " 'fleec',\n",
       " 'fli',\n",
       " 'floral',\n",
       " 'florita',\n",
       " 'flower',\n",
       " 'flute',\n",
       " 'flutter',\n",
       " 'forev',\n",
       " 'four',\n",
       " 'fox',\n",
       " 'frame',\n",
       " 'franci',\n",
       " 'francois',\n",
       " 'fray',\n",
       " 'freestyl',\n",
       " 'freja',\n",
       " 'french',\n",
       " 'fring',\n",
       " 'front',\n",
       " 'full',\n",
       " 'funnel',\n",
       " 'fur',\n",
       " 'futur',\n",
       " 'gabardin',\n",
       " 'gabriella',\n",
       " 'galativi',\n",
       " 'gansu',\n",
       " 'garden',\n",
       " 'gather',\n",
       " 'gauz',\n",
       " 'geo',\n",
       " 'georgett',\n",
       " 'gerda',\n",
       " 'getaway',\n",
       " 'gingham',\n",
       " 'girl',\n",
       " 'glimps',\n",
       " 'gloria',\n",
       " 'gloss',\n",
       " 'gold',\n",
       " 'goldi',\n",
       " 'grain',\n",
       " 'grate',\n",
       " 'grosgrain',\n",
       " 'gwen',\n",
       " 'haight',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halo',\n",
       " 'halterneck',\n",
       " 'halyn',\n",
       " 'hammock',\n",
       " 'handl',\n",
       " 'handprint',\n",
       " 'hanro',\n",
       " 'harem',\n",
       " 'harlow',\n",
       " 'harlyn',\n",
       " 'harper',\n",
       " 'harriett',\n",
       " 'hatch',\n",
       " 'haze',\n",
       " 'hazina',\n",
       " 'heart',\n",
       " 'heather',\n",
       " 'heel',\n",
       " 'helen',\n",
       " 'hem',\n",
       " 'hendrix',\n",
       " 'henley',\n",
       " 'herringbon',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'holden',\n",
       " 'hood',\n",
       " 'hoodi',\n",
       " 'hopper',\n",
       " 'hortensia',\n",
       " 'houndstooth',\n",
       " 'hour',\n",
       " 'hybridg',\n",
       " 'hyper',\n",
       " 'ibiza',\n",
       " 'idun',\n",
       " 'iggi',\n",
       " 'imagin',\n",
       " 'imanea',\n",
       " 'impact',\n",
       " 'indiannah',\n",
       " 'inset',\n",
       " 'intarsia',\n",
       " 'interv',\n",
       " 'isa',\n",
       " 'italian',\n",
       " 'ivi',\n",
       " 'jacket',\n",
       " 'jacquard',\n",
       " 'jami',\n",
       " 'jane',\n",
       " 'janet',\n",
       " 'japan',\n",
       " 'jasmin',\n",
       " 'je',\n",
       " 'jean',\n",
       " 'jeann',\n",
       " 'jenna',\n",
       " 'jennina',\n",
       " 'jerri',\n",
       " 'jersey',\n",
       " 'jessalyn',\n",
       " 'jewel',\n",
       " 'jimi',\n",
       " 'joan',\n",
       " 'jogger',\n",
       " 'josephin',\n",
       " 'journey',\n",
       " 'juan',\n",
       " 'julian',\n",
       " 'jumpsuit',\n",
       " 'juna',\n",
       " 'kaia',\n",
       " 'kalita',\n",
       " 'kaminski',\n",
       " 'karli',\n",
       " 'kate',\n",
       " 'kelli',\n",
       " 'kendal',\n",
       " 'keyhol',\n",
       " 'khiva',\n",
       " 'kicker',\n",
       " 'kisa',\n",
       " 'kitten',\n",
       " 'kitti',\n",
       " 'knee',\n",
       " 'knightley',\n",
       " 'knit',\n",
       " 'knot',\n",
       " 'lace',\n",
       " 'ladi',\n",
       " 'laney',\n",
       " 'lang',\n",
       " 'lanica',\n",
       " 'larel',\n",
       " 'larg',\n",
       " 'larina',\n",
       " 'lashbi',\n",
       " 'latt',\n",
       " 'lauren',\n",
       " 'law',\n",
       " 'lawson',\n",
       " 'layer',\n",
       " 'layla',\n",
       " 'lazo',\n",
       " 'le',\n",
       " 'lean',\n",
       " 'leandra',\n",
       " 'leather',\n",
       " 'leav',\n",
       " 'leg',\n",
       " 'legion',\n",
       " 'legit',\n",
       " 'leisur',\n",
       " 'lembongan',\n",
       " 'length',\n",
       " 'leo',\n",
       " 'leopard',\n",
       " 'leopold',\n",
       " 'lewi',\n",
       " 'liberti',\n",
       " 'lila',\n",
       " 'lilah',\n",
       " 'lilliana',\n",
       " 'limit',\n",
       " 'lina',\n",
       " 'line',\n",
       " 'linen',\n",
       " 'linett',\n",
       " 'link',\n",
       " 'lion',\n",
       " 'lisetteti',\n",
       " 'liv',\n",
       " 'lizard',\n",
       " 'loafer',\n",
       " 'loan',\n",
       " 'lock',\n",
       " 'logo',\n",
       " 'lola',\n",
       " 'london',\n",
       " 'long',\n",
       " 'loulou',\n",
       " 'loung',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lucil',\n",
       " 'luka',\n",
       " 'lurex',\n",
       " 'lustrou',\n",
       " 'lyocel',\n",
       " 'lyric',\n",
       " 'lystal',\n",
       " 'mackay',\n",
       " 'macram',\n",
       " 'madelyn',\n",
       " 'madison',\n",
       " 'madrid',\n",
       " 'maev',\n",
       " 'magal',\n",
       " 'magani',\n",
       " 'magdelena',\n",
       " 'mahali',\n",
       " 'maida',\n",
       " 'maika',\n",
       " 'makenna',\n",
       " 'manhattan',\n",
       " 'mankind',\n",
       " 'mannon',\n",
       " 'mantella',\n",
       " 'manu',\n",
       " 'marci',\n",
       " 'marco',\n",
       " 'margot',\n",
       " 'marguerit',\n",
       " 'mari',\n",
       " 'marwa',\n",
       " 'matelass',\n",
       " 'mathewson',\n",
       " 'matt',\n",
       " 'mattea',\n",
       " 'maven',\n",
       " 'maxi',\n",
       " 'maxin',\n",
       " 'medium',\n",
       " 'mel',\n",
       " 'mellon',\n",
       " 'melvin',\n",
       " 'mend',\n",
       " 'mercantil',\n",
       " 'meredith',\n",
       " 'merino',\n",
       " 'mesh',\n",
       " 'mesmer',\n",
       " 'meta',\n",
       " 'metal',\n",
       " 'metropolitan',\n",
       " 'mia',\n",
       " 'micro',\n",
       " 'microfleec',\n",
       " 'mid',\n",
       " 'midi',\n",
       " 'midris',\n",
       " 'milan',\n",
       " 'mile',\n",
       " 'militari',\n",
       " 'milla',\n",
       " 'milo',\n",
       " 'mini',\n",
       " 'minidress',\n",
       " 'minut',\n",
       " 'miranda',\n",
       " 'mirella',\n",
       " 'mirror',\n",
       " 'missoni',\n",
       " 'mitsu',\n",
       " 'mock',\n",
       " 'mockneck',\n",
       " 'modal',\n",
       " 'modern',\n",
       " 'mohair',\n",
       " 'moir',\n",
       " 'momentum',\n",
       " 'monogram',\n",
       " 'moreau',\n",
       " 'moto',\n",
       " 'move',\n",
       " 'moxi',\n",
       " 'mulberri',\n",
       " 'mule',\n",
       " 'multi',\n",
       " 'muscl',\n",
       " 'museo',\n",
       " 'nanci',\n",
       " 'nari',\n",
       " 'natti',\n",
       " 'navi',\n",
       " 'ne',\n",
       " 'neaj',\n",
       " 'nearlynud',\n",
       " 'neck',\n",
       " 'necklin',\n",
       " 'needl',\n",
       " 'net',\n",
       " 'nevelson',\n",
       " 'new',\n",
       " 'nichola',\n",
       " 'nik',\n",
       " 'noir',\n",
       " 'nubuck',\n",
       " 'nudistsong',\n",
       " 'oak',\n",
       " 'olga',\n",
       " 'olivia',\n",
       " 'one',\n",
       " 'open',\n",
       " 'organ',\n",
       " 'organza',\n",
       " 'origin',\n",
       " 'orion',\n",
       " 'ossi',\n",
       " 'otto',\n",
       " 'over',\n",
       " 'overal',\n",
       " 'overlay',\n",
       " 'overs',\n",
       " 'packabl',\n",
       " 'paco',\n",
       " 'painter',\n",
       " 'paisley',\n",
       " 'palazzo',\n",
       " 'panel',\n",
       " 'pant',\n",
       " 'pari',\n",
       " 'park',\n",
       " 'parker',\n",
       " 'parti',\n",
       " 'pascal',\n",
       " 'patchwork',\n",
       " 'patent',\n",
       " 'pattern',\n",
       " 'paula',\n",
       " 'pch',\n",
       " 'pearl',\n",
       " 'peasant',\n",
       " 'pencil',\n",
       " 'penni',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'pernil',\n",
       " 'petit',\n",
       " 'phoeb',\n",
       " 'photo',\n",
       " 'pia',\n",
       " 'picasso',\n",
       " 'piec',\n",
       " 'pima',\n",
       " 'pinstrip',\n",
       " 'piper',\n",
       " 'piqu',\n",
       " 'plaid',\n",
       " 'platform',\n",
       " 'player',\n",
       " 'pleat',\n",
       " 'plexi',\n",
       " 'pliss',\n",
       " 'plutu',\n",
       " 'pocket',\n",
       " 'point',\n",
       " 'pointel',\n",
       " 'pointi',\n",
       " 'polar',\n",
       " 'polka',\n",
       " 'poncho',\n",
       " 'pont',\n",
       " 'pop',\n",
       " 'poplin',\n",
       " 'popov',\n",
       " 'porter',\n",
       " 'portofino',\n",
       " 'post',\n",
       " 'pouch',\n",
       " 'poudr',\n",
       " 'princ',\n",
       " 'print',\n",
       " 'prism',\n",
       " 'pristin',\n",
       " 'puff',\n",
       " 'puffer',\n",
       " 'pull',\n",
       " 'pullov',\n",
       " 'pump',\n",
       " 'purist',\n",
       " 'pussi',\n",
       " 'pvc',\n",
       " 'python',\n",
       " 'quarter',\n",
       " 'queeni',\n",
       " 'quoi',\n",
       " 'rae',\n",
       " 'raffia',\n",
       " 'rami',\n",
       " 'raquel',\n",
       " 'ratti',\n",
       " 'rav',\n",
       " 'ravello',\n",
       " 'raw',\n",
       " 'realiti',\n",
       " 'remov',\n",
       " 'resist',\n",
       " 'retro',\n",
       " 'revel',\n",
       " 'revers',\n",
       " 'rib',\n",
       " 'rigid',\n",
       " 'riley',\n",
       " 'rip',\n",
       " 'rise',\n",
       " 'rivello',\n",
       " 'riviera',\n",
       " 'roe',\n",
       " 'roll',\n",
       " 'romi',\n",
       " 'romper',\n",
       " 'rosali',\n",
       " 'rosendo',\n",
       " 'rossana',\n",
       " 'rubber',\n",
       " 'rubi',\n",
       " 'ruch',\n",
       " 'ruffl',\n",
       " 'ryan',\n",
       " 'ryle',\n",
       " 'saddl',\n",
       " 'sadi',\n",
       " 'sai',\n",
       " 'samantha',\n",
       " 'sami',\n",
       " 'sandal',\n",
       " 'sanjit',\n",
       " 'satchel',\n",
       " 'satin',\n",
       " 'saturday',\n",
       " 'saturn',\n",
       " 'scarf',\n",
       " 'schoolboy',\n",
       " 'scolto',\n",
       " 'scoop',\n",
       " 'scoopneck',\n",
       " 'sea',\n",
       " 'season',\n",
       " 'second',\n",
       " 'selena',\n",
       " 'self',\n",
       " 'sensillo',\n",
       " 'sequin',\n",
       " 'seraya',\n",
       " 'shadow',\n",
       " 'shaker',\n",
       " 'shawl',\n",
       " 'shay',\n",
       " 'shearl',\n",
       " 'sheath',\n",
       " 'sheer',\n",
       " 'shell',\n",
       " 'shift',\n",
       " 'shima',\n",
       " 'shirley',\n",
       " 'shirt',\n",
       " 'shirtdress',\n",
       " 'shoe',\n",
       " 'short',\n",
       " 'shoulder',\n",
       " 'shrunken',\n",
       " 'side',\n",
       " 'sierra',\n",
       " 'sigantur',\n",
       " 'silk',\n",
       " 'singl',\n",
       " 'skinni',\n",
       " 'skirt',\n",
       " 'sleepi',\n",
       " 'sleev',\n",
       " 'sleeveless',\n",
       " 'slender',\n",
       " 'slide',\n",
       " 'slim',\n",
       " 'slingback',\n",
       " 'slip',\n",
       " 'slipdress',\n",
       " 'slit',\n",
       " 'slouch',\n",
       " 'slub',\n",
       " 'small',\n",
       " 'smock',\n",
       " 'smooth',\n",
       " 'snake',\n",
       " 'snakeskin',\n",
       " 'sneaker',\n",
       " 'soa',\n",
       " 'soft',\n",
       " 'soho',\n",
       " 'sophia',\n",
       " 'soutach',\n",
       " 'spaghetti',\n",
       " 'special',\n",
       " 'spike',\n",
       " 'split',\n",
       " 'spong',\n",
       " 'sport',\n",
       " 'spot',\n",
       " 'squar',\n",
       " 'sraight',\n",
       " 'st',\n",
       " 'stadium',\n",
       " 'stand',\n",
       " 'star',\n",
       " 'stella',\n",
       " 'stitch',\n",
       " 'stone',\n",
       " 'straight',\n",
       " 'strap',\n",
       " 'strappi',\n",
       " 'street',\n",
       " 'stretch',\n",
       " 'strike',\n",
       " 'string',\n",
       " 'stripe',\n",
       " 'stud',\n",
       " 'studio',\n",
       " 'style',\n",
       " 'sued',\n",
       " 'suit',\n",
       " 'sum',\n",
       " 'sundri',\n",
       " 'sunflow',\n",
       " 'super',\n",
       " 'supercozi',\n",
       " 'supersoft',\n",
       " 'superstar',\n",
       " 'supima',\n",
       " 'supplex',\n",
       " 'support',\n",
       " 'sur',\n",
       " 'susan',\n",
       " 'sustain',\n",
       " 'sutherland',\n",
       " 'sweat',\n",
       " 'sweater',\n",
       " 'sweatpant',\n",
       " 'sweatshirt',\n",
       " 'sweetheart',\n",
       " 'sweeti',\n",
       " 'swiss',\n",
       " 'sylvi',\n",
       " 'tailor',\n",
       " 'tall',\n",
       " 'tamara',\n",
       " 'tango',\n",
       " 'tank',\n",
       " 'tanya',\n",
       " 'tao',\n",
       " 'taper',\n",
       " 'tariana',\n",
       " 'tartan',\n",
       " 'taylor',\n",
       " 'teah',\n",
       " 'tee',\n",
       " 'teisbaek',\n",
       " 'tencel',\n",
       " 'terri',\n",
       " 'textur',\n",
       " 'theron',\n",
       " 'thong',\n",
       " 'tie',\n",
       " 'tier',\n",
       " 'tiger',\n",
       " 'tight',\n",
       " 'tinsel',\n",
       " 'tissu',\n",
       " 'to',\n",
       " 'toe',\n",
       " 'tomcat',\n",
       " 'ton',\n",
       " 'tone',\n",
       " 'toni',\n",
       " 'toothpick',\n",
       " 'top',\n",
       " 'tori',\n",
       " 'tortois',\n",
       " 'tote',\n",
       " 'touch',\n",
       " 'tour',\n",
       " 'tournament',\n",
       " 'tove',\n",
       " 'track',\n",
       " 'transpar',\n",
       " 'trench',\n",
       " 'triangl',\n",
       " 'trim',\n",
       " 'tropic',\n",
       " 'trouser',\n",
       " 'true',\n",
       " 'tulip',\n",
       " 'tull',\n",
       " 'tunic',\n",
       " 'tunnel',\n",
       " 'turtleneck',\n",
       " 'tuscan',\n",
       " 'tweed',\n",
       " 'twill',\n",
       " 'twist',\n",
       " 'two',\n",
       " 'ultraboost',\n",
       " 'ultralight',\n",
       " 'unlin',\n",
       " 'urban',\n",
       " 'us',\n",
       " 'util',\n",
       " 'vanessa',\n",
       " 'vapor',\n",
       " 'vegan',\n",
       " 'velour',\n",
       " 'velvet',\n",
       " 'versilia',\n",
       " 'vest',\n",
       " 'vill',\n",
       " 'vintag',\n",
       " 'vinyl',\n",
       " 'viola',\n",
       " 'violeta',\n",
       " 'viper',\n",
       " 'virgin',\n",
       " 'virgini',\n",
       " 'visa',\n",
       " 'voil',\n",
       " 'volumin',\n",
       " 'waffl',\n",
       " 'waist',\n",
       " 'wale',\n",
       " 'wallet',\n",
       " 'waltham',\n",
       " 'wash',\n",
       " 'washabl',\n",
       " 'water',\n",
       " 'way',\n",
       " 'weatherproof',\n",
       " 'welli',\n",
       " 'wendal',\n",
       " 'whisper',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'willow',\n",
       " 'women',\n",
       " 'wool',\n",
       " 'workwear',\n",
       " 'woven',\n",
       " 'wrap',\n",
       " 'xl',\n",
       " 'yara',\n",
       " 'yarn',\n",
       " 'yoke',\n",
       " 'young',\n",
       " 'zebra',\n",
       " 'zinnia',\n",
       " 'zip',\n",
       " 'zoeyi']"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,987) (2837,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-458-98c79bdee5b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_NB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred_NB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[1;32m    452\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[1;32m    453\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,987) (2837,) "
     ]
    }
   ],
   "source": [
    "y_pred_NB = classifier.predict(X.toarray())\n",
    "y_pred_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2262, 2837)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3393, 2837)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3393,)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
