{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Full+data.csv')\n",
    "extra=pd.read_csv('extra_data.csv')\n",
    "attributes=pd.read_excel('Womens+Attributes.xlsx')\n",
    "USC_product_attributes=pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx')\n",
    "attributes=attributes[['Style',\n",
    "       '(Reference Style Lookbook, choose all that apply)']]\n",
    "tags=pd.read_csv('usc_additional_tags.csv')\n",
    "#Dropped all columns with ALL null values\n",
    "data_dropped=data.dropna(axis=1,how='all')\n",
    "#Attach additional data to full data\n",
    "full_data=pd.concat([data_dropped,extra])\n",
    "full_data.drop_duplicates(subset=['product_id'], keep=\"first\",inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=full_data[['product_id','description','details','name','brand_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.fillna('UNKNOWNTOKEN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model=full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #See how many occasion exists, These are the occasions we need to look for and find rules for\n",
    "# final_data=final_data[final_data['attribute_name']=='occasion']\n",
    "# subset_for_model=final_data[['brand_category','description','details','name','product_id','attribute_value']]\n",
    "#Remove / from brand_category by space\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace(',',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('\\n',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace(',',' ')\n",
    "subset_for_model.head()\n",
    "subset_for_model.fillna('UNKNOWNTOKEN',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>name</th>\n",
       "      <th>brand_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSRPSZTDW2PGK1YWYXJGKZZ0</td>\n",
       "      <td>Vintage Fitness leather sneakers with logo pri...</td>\n",
       "      <td>TheMensStore Shoes Sneakers LowTop</td>\n",
       "      <td>Original Fitness Sneakers</td>\n",
       "      <td>TheMensStore Shoes Sneakers LowTop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DSQXJBX0R7DCW7KTAC1SW547</td>\n",
       "      <td>UNKNOWNTOKEN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>HAT</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DPGV8TGRAB993PF7Z3YWG2VR</td>\n",
       "      <td>A Timeless Leather Belt Crafted From Smooth Co...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Petit Oval Buckle Belt</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DSR8G3F7DBRTMP8THF97XSQ2</td>\n",
       "      <td>Pretty ruffle sleeves and trim elevate essenti...</td>\n",
       "      <td>JustKids Girls214 Girls SwimwearCoverups JustK...</td>\n",
       "      <td>Little Gir's &amp; Girl's Ariana One-Piece UPF 50+...</td>\n",
       "      <td>JustKids Girls214 Girls SwimwearCoverups JustK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DSR8G5GP519DEDCSKBMWQVK5</td>\n",
       "      <td>Versatile convertible gown with elephant applique</td>\n",
       "      <td>JustKids Baby024months InfantGirls FootiesRompers</td>\n",
       "      <td>Baby Girl's Endearing Elephants Pima Cotton Co...</td>\n",
       "      <td>JustKids Baby024months InfantGirls FootiesRompers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id  \\\n",
       "0  01DSRPSZTDW2PGK1YWYXJGKZZ0   \n",
       "1  01DSQXJBX0R7DCW7KTAC1SW547   \n",
       "2  01DPGV8TGRAB993PF7Z3YWG2VR   \n",
       "3  01DSR8G3F7DBRTMP8THF97XSQ2   \n",
       "4  01DSR8G5GP519DEDCSKBMWQVK5   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vintage Fitness leather sneakers with logo pri...   \n",
       "1                                       UNKNOWNTOKEN   \n",
       "2  A Timeless Leather Belt Crafted From Smooth Co...   \n",
       "3  Pretty ruffle sleeves and trim elevate essenti...   \n",
       "4  Versatile convertible gown with elephant applique   \n",
       "\n",
       "                                             details  \\\n",
       "0                 TheMensStore Shoes Sneakers LowTop   \n",
       "1                                            Unknown   \n",
       "2                                        Accessories   \n",
       "3  JustKids Girls214 Girls SwimwearCoverups JustK...   \n",
       "4  JustKids Baby024months InfantGirls FootiesRompers   \n",
       "\n",
       "                                                name  \\\n",
       "0                          Original Fitness Sneakers   \n",
       "1                                                HAT   \n",
       "2                             Petit Oval Buckle Belt   \n",
       "3  Little Gir's & Girl's Ariana One-Piece UPF 50+...   \n",
       "4  Baby Girl's Endearing Elephants Pima Cotton Co...   \n",
       "\n",
       "                                      brand_category  \n",
       "0                 TheMensStore Shoes Sneakers LowTop  \n",
       "1                                            Unknown  \n",
       "2                                        Accessories  \n",
       "3  JustKids Girls214 Girls SwimwearCoverups JustK...  \n",
       "4  JustKids Baby024months InfantGirls FootiesRompers  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_for_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id        0\n",
       "description       0\n",
       "details           0\n",
       "name              0\n",
       "brand_category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_for_model.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model=subset_for_model.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehilsaraswat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "columns=['details','description','name','brand_category']\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(subset_for_model)):\n",
    "        new=re.sub('[^a-zA-Z]', ' ',str(subset_for_model.loc[i,j]))\n",
    "        p=[]\n",
    "        for k in new.split():\n",
    "            separated=re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ',k)\n",
    "            new = separated.lower()\n",
    "            p.append(new)\n",
    "        ls=WordNetLemmatizer()\n",
    "        new = [ls.lemmatize(word) for word in p if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        subset_for_model.loc[i,j]=new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model['details']=subset_for_model['details'].str.replace('dressesandjumpsuits','dresses and jumpsuits')\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace('dressesandjumpsuits','dresses and jumpsuits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_product_attributes=pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx')\n",
    "new_tags=pd.concat([tags,USC_product_attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tags=new_tags[['product_id','attribute_name','attribute_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occ_tags=new_tags[new_tags['attribute_name']=='occasion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occ_tags=new_occ_tags.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occ_tags['attribute_value']=new_occ_tags['attribute_value'].str.lower()\n",
    "new_occ_tags['attribute_value']=new_occ_tags['attribute_value'].str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weekend', 'workout', 'daytonight', 'coldweather', 'vacation',\n",
       "       'nightout', 'work'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_occ_tags['attribute_value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we can see there can be multiple value so we create flag for each type of product_tag\n",
    "list_of_occasions = new_occ_tags['attribute_value'].unique()\n",
    "for j in list_of_occasions:\n",
    "    for i in range(0,len(new_occ_tags)):\n",
    "        if(new_occ_tags.loc[i,'attribute_value'] == j):\n",
    "            new_occ_tags.loc[i,'is_'+j] = 1\n",
    "        else:\n",
    "            new_occ_tags.loc[i,'is_'+j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>product_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_workout</th>\n",
       "      <th>is_daytonight</th>\n",
       "      <th>is_coldweather</th>\n",
       "      <th>is_vacation</th>\n",
       "      <th>is_nightout</th>\n",
       "      <th>is_work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31579</td>\n",
       "      <td>01E2P1MQS0GQ4SE93SY68ZYMCC</td>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31580</td>\n",
       "      <td>01E2P1MQS0GQ4SE93SY68ZYMCC</td>\n",
       "      <td>occasion</td>\n",
       "      <td>weekend</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31605</td>\n",
       "      <td>01E2P2QPCCQAN8WCHQKZY7EG73</td>\n",
       "      <td>occasion</td>\n",
       "      <td>workout</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31611</td>\n",
       "      <td>01E2P2Q2WXXD75B1F4JCAKC99D</td>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31612</td>\n",
       "      <td>01E2P2Q2WXXD75B1F4JCAKC99D</td>\n",
       "      <td>occasion</td>\n",
       "      <td>daytonight</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  product_id attribute_name attribute_value  \\\n",
       "0  31579  01E2P1MQS0GQ4SE93SY68ZYMCC       occasion         weekend   \n",
       "1  31580  01E2P1MQS0GQ4SE93SY68ZYMCC       occasion         weekend   \n",
       "2  31605  01E2P2QPCCQAN8WCHQKZY7EG73       occasion         workout   \n",
       "3  31611  01E2P2Q2WXXD75B1F4JCAKC99D       occasion      daytonight   \n",
       "4  31612  01E2P2Q2WXXD75B1F4JCAKC99D       occasion      daytonight   \n",
       "\n",
       "   is_weekend  is_workout  is_daytonight  is_coldweather  is_vacation  \\\n",
       "0         1.0         0.0            0.0             0.0          0.0   \n",
       "1         1.0         0.0            0.0             0.0          0.0   \n",
       "2         0.0         1.0            0.0             0.0          0.0   \n",
       "3         0.0         0.0            1.0             0.0          0.0   \n",
       "4         0.0         0.0            1.0             0.0          0.0   \n",
       "\n",
       "   is_nightout  is_work  \n",
       "0          0.0      0.0  \n",
       "1          0.0      0.0  \n",
       "2          0.0      0.0  \n",
       "3          0.0      0.0  \n",
       "4          0.0      0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_occ_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "work=new_occ_tags[['product_id','is_work']]\n",
    "tagged_work=work.groupby(['product_id'])['is_work'].max().reset_index()\n",
    "\n",
    "nightout=new_occ_tags[['product_id','is_nightout']]\n",
    "tagged_nightout=nightout.groupby(['product_id'])['is_nightout'].max().reset_index()\n",
    "\n",
    "daytonight=new_occ_tags[['product_id','is_daytonight']]\n",
    "tagged_daytonight=daytonight.groupby(['product_id'])['is_daytonight'].max().reset_index()\n",
    "\n",
    "weekend=new_occ_tags[['product_id','is_weekend']]\n",
    "tagged_weekend=weekend.groupby(['product_id'])['is_weekend'].max().reset_index()\n",
    "\n",
    "vacation=new_occ_tags[['product_id','is_vacation']]\n",
    "tagged_vacation=vacation.groupby(['product_id'])['is_vacation'].max().reset_index()\n",
    "\n",
    "workout=new_occ_tags[['product_id','is_workout']]\n",
    "tagged_workout=workout.groupby(['product_id'])['is_workout'].max().reset_index()\n",
    "\n",
    "coldweather=new_occ_tags[['product_id','is_coldweather']]\n",
    "tagged_coldweather=coldweather.groupby(['product_id'])['is_coldweather'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, we need to add different tags one final tag df\n",
    "df_list = [tagged_nightout,tagged_daytonight,tagged_vacation,tagged_workout,tagged_weekend,tagged_coldweather]\n",
    "for df_ in df_list:\n",
    "    tagged_work = pd.merge(tagged_work, df_, on='product_id', how='left')\n",
    "tags_style_all = tagged_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling=pd.merge(subset_for_model,tags_style_all,on='product_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "modeling.to_csv('modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=['is_work', 'is_nightout', 'is_daytonight', 'is_vacation', 'is_workout',\n",
    "       'is_weekend', 'is_coldweather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = TfidfVectorizer(min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "49\n",
      "1033\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "columns=['brand_category','details','description','name']\n",
    "e=pd.DataFrame()\n",
    "for i in columns:\n",
    "    corpus = list(modeling[i].values)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    print(len(terms))\n",
    "    c=pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "    e=pd.concat([e,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=e.T.values\n",
    "y=modeling['is_work'].values\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7164750957854407\n"
     ]
    }
   ],
   "source": [
    "logreg=LogisticRegression(n_jobs=1, C=0.01)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3428717424629535"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling['is_work'].sum()/len(modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=['is_work', 'is_nightout', 'is_daytonight', 'is_vacation', 'is_workout',\n",
    "       'is_weekend', 'is_coldweather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7164750957854407 for tag is_work\n",
      "% for  is_work is 0.6571282575370465\n",
      "accuracy 0.7662835249042146 for tag is_nightout\n",
      "% for  is_nightout is 0.7669902912621359\n",
      "accuracy 0.722860791826309 for tag is_daytonight\n",
      "% for  is_daytonight is 0.29790495656617266\n",
      "accuracy 0.8518518518518519 for tag is_vacation\n",
      "% for  is_vacation is 0.8346959632089934\n",
      "accuracy 0.9553001277139208 for tag is_workout\n",
      "% for  is_workout is 0.9601430761369443\n",
      "accuracy 0.7816091954022989 for tag is_weekend\n",
      "% for  is_weekend is 0.2350536535513541\n",
      "accuracy 0.9246487867177522 for tag is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052\n"
     ]
    }
   ],
   "source": [
    "X=e.T.values\n",
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    logreg=LogisticRegression(n_jobs=1, C=0.01)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print('accuracy %s' % accuracy_score(y_test,y_pred),\"for tag\",i)\n",
    "    print(\"% for \", i, \"is\",1-(modeling[i].sum()/len(modeling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7803320561941252 (128, 64) is_work\n",
      "% for  is_work is 0.6571282575370465 0.3428717424629535 \n",
      "\n",
      "0.7650063856960408 (100, 50) is_work\n",
      "% for  is_work is 0.6571282575370465 0.3428717424629535 \n",
      "\n",
      "0.789272030651341 (50, 20) is_work\n",
      "% for  is_work is 0.6571282575370465 0.3428717424629535 \n",
      "\n",
      "0.768837803320562 (128, 64) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 0.23300970873786409 \n",
      "\n",
      "0.768837803320562 (100, 50) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 0.23300970873786409 \n",
      "\n",
      "0.7650063856960408 (50, 20) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 0.23300970873786409 \n",
      "\n",
      "0.7215836526181354 (128, 64) is_daytonight\n",
      "% for  is_daytonight is 0.29790495656617266 0.7020950434338273 \n",
      "\n",
      "0.7139208173690932 (100, 50) is_daytonight\n",
      "% for  is_daytonight is 0.29790495656617266 0.7020950434338273 \n",
      "\n",
      "0.7049808429118773 (50, 20) is_daytonight\n",
      "% for  is_daytonight is 0.29790495656617266 0.7020950434338273 \n",
      "\n",
      "0.842911877394636 (128, 64) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 0.16530403679100664 \n",
      "\n",
      "0.8365261813537676 (100, 50) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 0.16530403679100664 \n",
      "\n",
      "0.8352490421455939 (50, 20) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 0.16530403679100664 \n",
      "\n",
      "0.9680715197956578 (128, 64) is_workout\n",
      "% for  is_workout is 0.9601430761369443 0.0398569238630557 \n",
      "\n",
      "0.9693486590038314 (100, 50) is_workout\n",
      "% for  is_workout is 0.9601430761369443 0.0398569238630557 \n",
      "\n",
      "0.9655172413793104 (50, 20) is_workout\n",
      "% for  is_workout is 0.9601430761369443 0.0398569238630557 \n",
      "\n",
      "0.7816091954022989 (128, 64) is_weekend\n",
      "% for  is_weekend is 0.2350536535513541 0.7649463464486459 \n",
      "\n",
      "0.7713920817369093 (100, 50) is_weekend\n",
      "% for  is_weekend is 0.2350536535513541 0.7649463464486459 \n",
      "\n",
      "0.7650063856960408 (50, 20) is_weekend\n",
      "% for  is_weekend is 0.2350536535513541 0.7649463464486459 \n",
      "\n",
      "0.9348659003831418 (128, 64) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 0.06515074092999489 \n",
      "\n",
      "0.9246487867177522 (100, 50) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 0.06515074092999489 \n",
      "\n",
      "0.9374201787994891 (50, 20) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 0.06515074092999489 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    size=[(128,64),(100,50),(50,20)]\n",
    "    for j in size:\n",
    "            nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=j).fit(X_train, y_train)\n",
    "            y_pred_RF = nn.predict(X_test)\n",
    "            #print(\"Actual % of the data for tag\",p,\"is\",sum(subset_for_model['attribute_value'])/len(subset_for_model)*100)\n",
    "            print(accuracy_score(y_test,y_pred_RF),j,i)\n",
    "            print(\"% for \", i, \"is\",1-(modeling[i].sum()/len(modeling)),(modeling[i].sum()/len(modeling)),'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2410"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.where(subset_for_model['attribute_value']=='work',1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NB = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1473414477898783"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  78.67177522349937\n",
      "Random % for  is_work is 0.6571282575370465 \n",
      "\n",
      "SVM Accuracy Score ->  77.2669220945083\n",
      "Random % for  is_nightout is 0.7669902912621359 \n",
      "\n",
      "SVM Accuracy Score ->  74.07407407407408\n",
      "Random % for  is_daytonight is 0.7020950434338273 \n",
      "\n",
      "SVM Accuracy Score ->  86.71775223499361\n",
      "Random % for  is_vacation is 0.8346959632089934 \n",
      "\n",
      "SVM Accuracy Score ->  95.40229885057471\n",
      "Random % for  is_workout is 0.9601430761369443 \n",
      "\n",
      "SVM Accuracy Score ->  79.05491698595148\n",
      "Random % for  is_weekend is 0.7649463464486459 \n",
      "\n",
      "SVM Accuracy Score ->  92.33716475095785\n",
      "Random % for  is_coldweather is 0.9348492590700052 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    SVM = svm.SVC(C=0.1, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "    print(\"Random % for \", i, \"is\",max((1-(modeling[i].sum()/len(modeling))),(modeling[i].sum()/len(modeling))),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"desktop/nlp/project/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-44bfee24c17f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mislice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13030\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wv' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=modeling[['description','details','name','brand_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "names=['description','name','details','brand_category']\n",
    "c=pd.DataFrame()\n",
    "for i in names:\n",
    "    X_new = X.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "    X_word_average = word_averaging_list(wv,X_new)\n",
    "    X_array=pd.DataFrame(X_word_average)\n",
    "    c=pd.concat([c,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3914, 1200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7547892720306514 (128, 64) is_work\n",
      "% for  is_work is 0.6571282575370465 \n",
      "\n",
      "0.7484035759897829 (100, 50) is_work\n",
      "% for  is_work is 0.6571282575370465 \n",
      "\n",
      "0.7650063856960408 (50, 20) is_work\n",
      "% for  is_work is 0.6571282575370465 \n",
      "\n",
      "0.7420178799489144 (128, 64) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 \n",
      "\n",
      "0.7547892720306514 (100, 50) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 \n",
      "\n",
      "0.7381864623243933 (50, 20) is_nightout\n",
      "% for  is_nightout is 0.7669902912621359 \n",
      "\n",
      "0.7151979565772669 (128, 64) is_daytonight\n",
      "% for  is_daytonight is 0.7020950434338273 \n",
      "\n",
      "0.7203065134099617 (100, 50) is_daytonight\n",
      "% for  is_daytonight is 0.7020950434338273 \n",
      "\n",
      "0.7241379310344828 (50, 20) is_daytonight\n",
      "% for  is_daytonight is 0.7020950434338273 \n",
      "\n",
      "0.8518518518518519 (128, 64) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 \n",
      "\n",
      "0.8620689655172413 (100, 50) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 \n",
      "\n",
      "0.8403575989782887 (50, 20) is_vacation\n",
      "% for  is_vacation is 0.8346959632089934 \n",
      "\n",
      "0.9578544061302682 (128, 64) is_workout\n",
      "% for  is_workout is 0.9601430761369443 \n",
      "\n",
      "0.9553001277139208 (100, 50) is_workout\n",
      "% for  is_workout is 0.9601430761369443 \n",
      "\n",
      "0.9604086845466155 (50, 20) is_workout\n",
      "% for  is_workout is 0.9601430761369443 \n",
      "\n",
      "0.7943805874840357 (128, 64) is_weekend\n",
      "% for  is_weekend is 0.7649463464486459 \n",
      "\n",
      "0.7713920817369093 (100, 50) is_weekend\n",
      "% for  is_weekend is 0.7649463464486459 \n",
      "\n",
      "0.7713920817369093 (50, 20) is_weekend\n",
      "% for  is_weekend is 0.7649463464486459 \n",
      "\n",
      "0.9233716475095786 (128, 64) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 \n",
      "\n",
      "0.9284802043422733 (100, 50) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 \n",
      "\n",
      "0.913154533844189 (50, 20) is_coldweather\n",
      "% for  is_coldweather is 0.9348492590700052 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(c.values, y, test_size = 0.2, random_state = 0)\n",
    "    size=[(128,64),(100,50),(50,20)]\n",
    "    for j in size:\n",
    "        nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=j).fit(X_train, y_train)\n",
    "        y_pred_RF = nn.predict(X_test)\n",
    "        #print(\"Actual % of the data for tag\",p,\"is\",sum(subset_for_model['attribute_value'])/len(subset_for_model)*100)\n",
    "        print(accuracy_score(y_test,y_pred_RF),j,i)\n",
    "        print(\"% for \", i, \"is\",max((1-(modeling[i].sum()/len(modeling))),(modeling[i].sum()/len(modeling))),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  75.86206896551724\n",
      "Random % for  is_work is 0.6571282575370465 \n",
      "\n",
      "SVM Accuracy Score ->  76.62835249042146\n",
      "Random % for  is_nightout is 0.7669902912621359 \n",
      "\n",
      "SVM Accuracy Score ->  73.5632183908046\n",
      "Random % for  is_daytonight is 0.7020950434338273 \n",
      "\n",
      "SVM Accuracy Score ->  85.18518518518519\n",
      "Random % for  is_vacation is 0.8346959632089934 \n",
      "\n",
      "SVM Accuracy Score ->  95.53001277139208\n",
      "Random % for  is_workout is 0.9601430761369443 \n",
      "\n",
      "SVM Accuracy Score ->  78.16091954022988\n",
      "Random % for  is_weekend is 0.7649463464486459 \n",
      "\n",
      "SVM Accuracy Score ->  92.46487867177522\n",
      "Random % for  is_coldweather is 0.9348492590700052 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(c.values, y, test_size = 0.2, random_state = 0)\n",
    "    SVM = svm.SVC(C=0.1, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "    print(\"Random % for \", i, \"is\",max((1-(modeling[i].sum()/len(modeling))),(modeling[i].sum()/len(modeling))),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source of the code logic - https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Keras with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Word2Vec vectors in KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8400 for is_work\n",
      "Testing Accuracy:  0.8008 for is_work\n",
      "Training Accuracy: 0.8569 for is_nightout\n",
      "Testing Accuracy:  0.7765 for is_nightout\n",
      "Training Accuracy: 0.8330 for is_daytonight\n",
      "Testing Accuracy:  0.7676 for is_daytonight\n",
      "Training Accuracy: 0.8952 for is_vacation\n",
      "Testing Accuracy:  0.8863 for is_vacation\n",
      "Training Accuracy: 0.9939 for is_workout\n",
      "Testing Accuracy:  0.9681 for is_workout\n",
      "Training Accuracy: 0.8323 for is_weekend\n",
      "Testing Accuracy:  0.8008 for is_weekend\n",
      "Training Accuracy: 0.9559 for is_coldweather\n",
      "Testing Accuracy:  0.9361 for is_coldweather\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(c.values, y, test_size = 0.2, random_state = 0)\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=20,verbose=False,validation_data=(X_test, y_test),batch_size=10)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy),\"for\",i)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy),\"for\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This simple Keras Sequential Model with word2Vec does better than Rest of the models for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras with our own embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling=pd.read_csv('modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling['all_data']=modeling['details']+modeling['name']+modeling['description']+modeling['brand_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "columns=['brand_category','details','description','name']\n",
    "for i in ['brand_category']:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(modeling[i])\n",
    "    X = tokenizer.texts_to_sequences(modeling[i])\n",
    "    # Adding 1 because of reserved 0 index\n",
    "    CD=tokenizer.word_index\n",
    "    maxlen = 100\n",
    "    X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "for i in ['details','description','name']:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(modeling[i])\n",
    "    p = tokenizer.texts_to_sequences(modeling[i])# Adding 1 because of reserved 0 index\n",
    "    CD={**CD,**tokenizer.word_index}\n",
    "    vocab_size=len(CD)\n",
    "    p = pad_sequences(p, padding='post', maxlen=maxlen)\n",
    "    X=np.concatenate((X,p),axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(modeling['all_data'])\n",
    "X = tokenizer.texts_to_sequences(modeling['all_data'])\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "maxlen = 1000\n",
    "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "# for i in ['details','description','name']:\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(modeling[i])\n",
    "#     p = tokenizer.texts_to_sequences(modeling[i])# Adding 1 because of reserved 0 index\n",
    "#     CD={**CD,**tokenizer.word_index}\n",
    "#     vocab_size=len(CD)\n",
    "#     p = pad_sequences(p, padding='post', maxlen=maxlen)\n",
    "#     X=np.concatenate((X,p),axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11152"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_154\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 1000, 50)          557600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_18 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_305 (Dense)            (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 558,121\n",
      "Trainable params: 558,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6509 for is_work\n",
      "Testing Accuracy:  0.6897 for is_work\n",
      "Training Accuracy: 0.7681 for is_nightout\n",
      "Testing Accuracy:  0.7637 for is_nightout\n",
      "Training Accuracy: 0.7055 for is_daytonight\n",
      "Testing Accuracy:  0.6922 for is_daytonight\n",
      "Training Accuracy: 0.8333 for is_vacation\n",
      "Testing Accuracy:  0.8506 for is_vacation\n",
      "Training Accuracy: 0.9614 for is_workout\n",
      "Testing Accuracy:  0.9540 for is_workout\n",
      "Training Accuracy: 0.7566 for is_weekend\n",
      "Testing Accuracy:  0.7791 for is_weekend\n",
      "Training Accuracy: 0.9384 for is_coldweather\n",
      "Testing Accuracy:  0.9234 for is_coldweather\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10,verbose=False,validation_data=(X_test, y_test),batch_size=5)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy),\"for\",i)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy),\"for\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras with Pre-trained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('desktop/nlp/project/glove.6B.300d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4425215208034433"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#34% of vocabulary is covered through pre trained embeddings\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 1000, 50)          557600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 558,121\n",
      "Trainable params: 521\n",
      "Non-trainable params: 557,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6573 for is_work\n",
      "Testing Accuracy:  0.6909 for is_work\n",
      "Training Accuracy: 0.7697 for is_nightout\n",
      "Testing Accuracy:  0.7612 for is_nightout\n",
      "Training Accuracy: 0.7189 for is_daytonight\n",
      "Testing Accuracy:  0.6833 for is_daytonight\n",
      "Training Accuracy: 0.8333 for is_vacation\n",
      "Testing Accuracy:  0.8480 for is_vacation\n",
      "Training Accuracy: 0.9617 for is_workout\n",
      "Testing Accuracy:  0.9553 for is_workout\n",
      "Training Accuracy: 0.7643 for is_weekend\n",
      "Testing Accuracy:  0.7752 for is_weekend\n",
      "Training Accuracy: 0.9371 for is_coldweather\n",
      "Testing Accuracy:  0.9132 for is_coldweather\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    y=modeling[i].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=20,verbose=False,validation_data=(X_test, y_test),batch_size=10)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy),\"for\",i)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy),\"for\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problems for low accuracy\n",
    "1. There are not enough training samples\n",
    "2. The data you have does not generalize well\n",
    "3. Missing focus on tweaking the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
